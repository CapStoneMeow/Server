import torch
from transformers import BertTokenizer, AutoModelForSequenceClassification

# âœ… ëª¨ë¸ ê²½ë¡œ
MODEL_PATH = "./model/kobert"

# âœ… ë¼ë²¨ ë§¤í•‘
label_map = {
    0: "ì´ˆë“±_ì €í•™ë…„",
    1: "ì´ˆë“±_ê³ í•™ë…„"
}

# âœ… ì „ì—­ ëª¨ë¸/í† í¬ë‚˜ì´ì € ê°ì²´
_tokenizer = None
_model = None

# âœ… ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
def load_model():
    global _tokenizer, _model
    if _tokenizer is None or _model is None:
        try:
            print("ðŸ“¦ KoBERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
            _tokenizer = BertTokenizer.from_pretrained(
                MODEL_PATH,
                local_files_only=True,
                trust_remote_code=True
            )
            _model = AutoModelForSequenceClassification.from_pretrained(
                MODEL_PATH,
                local_files_only=True,
                trust_remote_code=True
            )
            _model.eval()
            print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:", e)
            raise RuntimeError("ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

# âœ… ì˜ˆì¸¡ í•¨ìˆ˜
def predict_grade(text: str) -> dict:
    load_model()

    inputs = _tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding="max_length",
        max_length=128
    )

    with torch.no_grad():
        outputs = _model(**inputs)
        pred = torch.argmax(outputs.logits, dim=1).item()

    return {
        "label_index": pred,
        "label": label_map.get(pred, "Unknown")
    }
